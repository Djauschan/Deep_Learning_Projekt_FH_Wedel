directories:
  train_data: data/train_data.csv # Path to training data
  test_data: data/test_data.csv # Path to test data

agent_types: ['ma5', 'ma30', 'ma200', 'rsi']

aggregation:
  aggregation_state_size: 5 #Inputsize from each agent
  
q_models: # Path to q-models
  ma5: models/ma5_agent_q_table.npy
  ma30: models/ma5_agent_q_table.npy
  ma200: models/ma5_agent_q_table.npy
  rsi: models/rsi_agent_q_table.npy
  aggregation: models/aggregation_agent_q_table.npy


train_parameters:
  epochs: 120  # 150 # Number of epochs to train
  state_size: 20  # Number of different states that an agent can distinguish 
  action_size: 3   # Possible actions: sell (+1), hold (0), buy (+2)

  #monitor the Portfoliovalue over the trainingcycle:
  start_cash_monitoring: 1000 #monitor the cashvalue over the trainingcycle
  start_stock_monitoring: 0 #monitor the stockamount over the trainingcycle

  # parameters for Q-Learning Agents
  learning_rate: 0.2 # Learning rate
  discount_factor: 0.9 # Discount factor for future rewards
  exploration_rate: 0.2 # Initial exploration rate #0,2
  exploration_decay: 0.1 # Decay rate for the exploration rate #0,1
  exploration_min: 0.01 # Minimum exploration rate

  # parameters for rewards
  reward: 2 #correct prediction
  hold_reward: 0.0
  penalty: -2 #incorrect prediction

  #additional penalty, when the portfolio value undercut the min threshold
  min_portfolio_threshold: 0.3 #< 30% from the started value
  threshold_penalty: -1
  threshold_reward: 1

#Use manual Stopp loss orders:
stopp_loss:
  use_stopp_loss: FALSE #TRUE #FALSE
  buy_stop_loss_under_buy_price: 0.1 #0.1 Percent
  stopp_under_sell_price: 0.01 #0.01 Percent


traiding:
  startvalue: 10000







